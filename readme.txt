Here's an Airflow DAG written in Python that creates an Amazon EMR cluster, triggers Spark batch jobs on it daily,
and tears down the cluster once processing is complete.

### Steps to Set Up Airflow on AWS

Hereâ€™s a step-by-step guide to setting up Apache Airflow as a service on AWS, leveraging tools like **Amazon Managed Workflows for Apache Airflow (MWAA)** or self-managed setups.

---

#### **Option 1: Using Amazon MWAA (Managed Airflow)**

Amazon MWAA simplifies the deployment of Airflow by handling infrastructure and scaling for you.

1. **Create an S3 Bucket for Airflow Resources**:
   - Create an S3 bucket that will store your DAGs, plugins, and requirements files.
   - Folder structure example:
     ```
     dags/
     plugins/
     requirements.txt
     ```

2. **Prepare Your DAGs and Plugins**:
   - Upload your DAGs to the `dags/` folder.
   - If you need custom operators or libraries, upload them to the `plugins/` folder.
   - Add a `requirements.txt` file for any Python dependencies.

3. **Create an MWAA Environment**:
   - Navigate to the [AWS MWAA Console](https://console.aws.amazon.com/mwaa/).
   - Click **Create Environment** and configure:
     - **Execution Role**: Ensure the role has access to the S3 bucket and other AWS services like EMR.
     - **S3 Bucket**: Specify the bucket created earlier.
     - **Environment Variables**: Add any required environment variables.
     - **Networking**: Choose the VPC and subnets for the environment.

4. **Deploy the Environment**:
   - Wait for MWAA to provision the resources.
   - Access the Airflow UI from the MWAA console to verify the setup.

5. **Trigger and Monitor DAGs**:
   - In the Airflow UI, activate and monitor your DAGs.

---

#### **Option 2: Self-Managed Airflow on EC2**

If you prefer managing Airflow manually, you can deploy it on EC2.

1. **Launch an EC2 Instance**:
   - Choose an instance type (e.g., `t3.medium` or higher).
   - Attach an IAM role with access to S3, EMR, and other AWS services.

2. **Install Airflow**:
   - Install Python and pip:
     ```bash
     sudo yum install python3 -y
     pip3 install apache-airflow
     ```
   - Initialize Airflow:
     ```bash
     airflow db init
     airflow users create --username admin --password admin --role Admin --email admin@example.com
     ```

3. **Configure Airflow**:
   - Update the `airflow.cfg` file for your setup (e.g., configure executors, S3 paths).
   - Example S3 configuration:
     ```ini
     remote_base_log_folder = s3://your-bucket-name/airflow/logs
     remote_log_conn_id = aws_default
     ```

4. **Upload DAGs to the EC2 Instance**:
   - SCP your DAG files to the EC2 instance:
     ```bash
     scp your_dag.py ec2-user@<instance-ip>:/home/ec2-user/airflow/dags/
     ```

5. **Start Airflow Services**:
   - Start the Airflow scheduler and webserver:
     ```bash
     airflow scheduler &
     airflow webserver &
     ```

6. **Access the Airflow Web UI**:
   - Open the web server port (default: 8080) in the EC2 security group.
   - Access the UI at `http://<instance-ip>:8080`.

---

#### **Option 3: Deploy Airflow on Kubernetes (Optional)**

For scalability and robustness, you can deploy Airflow on Kubernetes using tools like Helm.

1. **Set Up an EKS Cluster**:
   - Use AWS EKS to provision a Kubernetes cluster.

2. **Install Helm and Airflow**:
   - Add the Apache Airflow Helm chart:
     ```bash
     helm repo add apache-airflow https://airflow.apache.org
     helm install airflow apache-airflow/airflow
     ```

3. **Configure DAGs and Storage**:
   - Mount an S3 bucket or EFS to store DAGs and logs.

4. **Scale and Monitor**:
   - Use Kubernetes autoscaling and monitoring tools for scaling.

---

Each approach has its pros and cons. For simplicity and AWS integration, **Amazon MWAA** is recommended.
 If you require full control, consider the EC2 or Kubernetes options. Let me know if you need detailed guidance on any step!



### Steps to Deploy the DAG:
1. **Set Up Airflow**:
   - Install Airflow using the recommended setup (e.g., with `pip` or Docker).
   - Install the required AWS providers: `pip install apache-airflow-providers-amazon`.

2. **Configure AWS Credentials**:
   - Add your AWS credentials to Airflow's `aws_default` connection in the UI or use IAM roles if running in AWS.

3. **Upload the DAG**:
   - Save the code in a file, e.g., `emr_spark_job_dag.py`, and place it in the Airflow DAGs folder (`~/airflow/dags` by default).

4. **Start Airflow**:
   - Start the Airflow web server and scheduler:
     ```bash
     airflow webserver &
     airflow scheduler &
     ```

5. **Activate the DAG**:
   - In the Airflow UI, activate the `emr_spark_job_dag`.

6. **Monitor Execution**:
   - Check the Airflow UI for task progress and logs.

Let me know if you need help customizing or deploying this!